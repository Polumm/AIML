# AIML

关键词：人工智能，机器学习，深度学习，算法实现，决策树，FCM，KMeans，KNN，LeNet，线性回归，逻辑回归，朴素贝叶斯，数据集，框架优化，一体化操作，算法性能。 

这个项目包含了人工智能与机器学习课程的源代码实现。在这个课程中，我研究了机器学习和深度学习的经典算法，包括线性回归，逻辑回归，决策树，KMeans（和FCM），KNN（和KDTree），朴素贝叶斯，LeNet等等，并在统一而方便的框架中实现并开源了这些算法。 支持的数据集包括Bouston，IndianPine，PaviaU，Iris，Mnist，和Mudou等。

在此项目中，我以决策树，FCM，KMeans，KNN，LeNet，线性回归，逻辑回归，朴素贝叶斯等经典算法为核心，详细研究了这些算法的基本原理和性能影响因素。对于每种算法，我探究了不同的参数、不同的数据集、是否采用性能优化算法等因素是如何影响算法的性能和结果。 此外，我优化了这个框架，使其更加一体化和方便使用。该项目有着较低的代码耦合性，便于修改完善和开展更丰富的实验。其支持批量实验，并且可以自动地保存实验参数和结果。通过这种自动化的训练模式，一定程度上减轻了科研工作者的实验负担。研究结果表明，对于常见的机器学习算法，合适的参数选择、优化算法的加入、适当的数据集划分比例可以提高算法的性能。 

------

Keywords: Artificial Intelligence, Machine Learning, Deep Learning, Algorithm Implementation, DecisionTree, FCM, KMeans, KNN, LeNet, Linear Regression, Logistic Regression, Naive Bayes, Datasets, Framework Optimization, Unified Operation, Algorithm Performance.

This project encompasses the source code implementation for the course of Artificial Intelligence and Machine Learning. In this course, I have explored classic algorithms of machine learning and deep learning, including Linear Regression, Logistic Regression, DecisionTree, KMeans (and FCM), KNN (and KDTree), Naive Bayes, LeNet, among others. These algorithms were implemented and open-sourced within a unified and convenient framework. Supported datasets include Bouston, IndianPine, PaviaU, Iris, Mnist, and Mudou, among others.

Within this project, I focused on the fundamental principles and performance-impacting factors of the classic algorithms such as DecisionTree, FCM, KMeans, KNN, LeNet, Linear Regression, Logistic Regression, and Naive Bayes. For each algorithm, I delved into how different parameters, diverse datasets, and the application of performance optimization methods influence the performance and results of the algorithms.

Furthermore, I have optimized the framework for greater integration and user-friendliness. This project has low code coupling, facilitates modification and enhancement, and fosters more extensive experimentation. It supports batch experiments and automatically saves experiment parameters and results. This automated training mode alleviates the experimental burden for researchers to a certain extent. Research results reveal that for common machine learning algorithms, appropriate parameter selection, integration of optimization algorithms, and suitable dataset split ratios can enhance algorithm performance.
